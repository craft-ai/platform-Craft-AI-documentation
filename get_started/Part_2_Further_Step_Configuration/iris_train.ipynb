{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 2: Execute a simple ML model - Further pipeline configuration"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from craft_ai_sdk import CraftAiSdk\n",
                "import dotenv\n",
                "import os\n",
                "import pandas as pd\n",
                "from sklearn import datasets\n",
                "\n",
                "dotenv.load_dotenv()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load environnement variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CRAFT_AI_SDK_TOKEN = os.environ.get(\"CRAFT_AI_SDK_TOKEN\")\n",
                "CRAFT_AI_ENVIRONMENT_URL = os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SDK instantiation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk = CraftAiSdk(sdk_token=CRAFT_AI_SDK_TOKEN, environment_url=CRAFT_AI_ENVIRONMENT_URL)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean Previous part"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can start by cleaning the objects we created in the hello world use case.\n",
                "\n",
                "To do so we can simply use the `delete_pipeline` function of the sdk. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.delete_pipeline(pipeline_name=\"part-1-hello-world\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload dataset to Data Store"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This use case uses the famous Iris dataset.\n",
                "\n",
                "With the Craft.AI platform, your environment comes with computational resources and file storage. That's what we call the **data store**.\n",
                "\n",
                "You can upload and download files and organize them using the SDK.\n",
                "\n",
                "We will start by uploading this dataset to the data store using the `upload_data_store_object` function of the sdk. \n",
                "\n",
                "You have to pass two arguments:\n",
                "- `filepath_or_buffer` : path of the file to be uploaded or a file-like object\n",
                "- `object_path_in_datastore`: path to save the file to\n",
                "\n",
                "You can find further information in the SDK documentation.\n",
                "\n",
                "In the following cell we first load the iris dataset from sklearn, then write it to the disk before uploading it to the data store. We then remove the file from the disk. You could avoid writing the dataset to the disk using file-like object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "iris = datasets.load_iris(as_frame=True)\n",
                "iris_df = pd.concat([iris.data, iris.target], axis=1)\n",
                "iris_df.to_parquet(\"iris.parquet\")\n",
                "\n",
                "sdk.upload_data_store_object(\n",
                "    filepath_or_buffer=\"iris.parquet\",\n",
                "    object_path_in_datastore=\"get_started/dataset/iris.parquet\",\n",
                ")\n",
                "\n",
                "os.remove(\"iris.parquet\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also check all the objects contained in the datastore using the `list_data_store_objects` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.list_data_store_objects()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And get information about a specific item with the `get_data_store_object_information` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.get_data_store_object_information(\"get_started/dataset/iris.parquet\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pipeline creation with the SDK"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create a pipeline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, it's time to create the **pipeline** embedding our code. \n",
                "\n",
                "We will do exactly what we have done previously in the *Hello_world* section, but this time we will use more advanced options.\n",
                "\n",
                "The argument `container_config` can contain other many things to parametrize our pipeline. \n",
                "Here we will focus on one new specific parameter:\n",
                "- `requirements_path`: in order for our `requirements.txt` file to be taken into account and therefore to onboard all necessary librairies in the pipeline, we add the relative path of this file in the `container_config`.\n",
                "\n",
                "You can find further information and configuration settings in the SDK documentation.\n",
                "\n",
                "The `requirements_path` can also be **specified by default** directly in your project, on the platform (in the Settings of your project). If you specify this argument in your pipeline creation, it will take it first into account before checking the values set at the project level.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.create_pipeline(\n",
                "    pipeline_name=\"part-2-iristrain\",\n",
                "    function_path=\"src/part-2-iris-train.py\",\n",
                "    function_name=\"trainIris\",\n",
                "    description=\"This function creates a classifier model for iris and makes prediction on test data set\",\n",
                "    container_config={\n",
                "        \"local_folder\": \"../../get_started\",\n",
                "        \"requirements_path\": \"requirements.txt\",\n",
                "    },\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run your Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.run_pipeline(pipeline_name=\"part-2-iristrain\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check model creation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can check the creation of the model by investigating the data store.\n",
                "\n",
                "Using the previously introduced `get_data_store_object_information`, we can easily verify that the model has been created and well uploaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdk.get_data_store_object_information(\"get_started/models/iris_knn_model.joblib\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, to clean the datastore, we use the `delete_data_store_object` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sdk.delete_data_store_object(\"get_started/models/iris_knn_model.joblib\")\n",
                "# sdk.delete_data_store_object(\"get_started/dataset/iris.parquet\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "232d0a11f8df5ff3a2ba2b13b61b4e3f653bd588275dad9272f8bdcbc6001c87"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
