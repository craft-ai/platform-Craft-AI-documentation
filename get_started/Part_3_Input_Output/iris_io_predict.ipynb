{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Execute a ML use case with inputs and outputs "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we have built and run our first ML pipeline to retrieve data from the data store, train a model and store it on the data store.\n",
    "\n",
    "What if we want to use our pipeline to perform predictions with this trained model on new data ? Currently, we can not pass new data to our model. ⇒ We need to add an Input to our pipeline.\n",
    "\n",
    "Moreover, what if we want to provide these predictions to a final user? ⇒ We need to add an Output to our pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from craft_ai_sdk import CraftAiSdk, Input, Output\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environnement variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAFT_AI_SDK_TOKEN = os.environ.get(\"CRAFT_AI_SDK_TOKEN\")\n",
    "CRAFT_AI_ENVIRONMENT_URL = os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk = CraftAiSdk(sdk_token=CRAFT_AI_SDK_TOKEN, environment_url=CRAFT_AI_ENVIRONMENT_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline creation with the SDK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s create our pipeline on the platform. Here, since we have inputs and outputs, our pipeline is the combination of three elements: inputs, outputs and the Python function. We will first declare the input and the output. Then, we will use the function sdk.create_pipeline() as in Part 2 to create the whole pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Input and Output\n",
    "\n",
    "To manage inputs and outputs of a pipeline, the platform requires you to declare them using the ``Input`` and ``Output`` classes from the SDK.\n",
    "\n",
    "For our Iris application, the inputs and outputs declaration would look like below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both objects have two main attributes:\n",
    "\n",
    "- The name\n",
    "- The data_type describing the type of data it can accept. It can be one of: ``string``, ``number``, ``boolean``, ``json``, ``array``.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input the name corresponds to the name of an argument of your pipeline's function. In our case name=\"input_data\" and \"input_model_path\" (as in the first line of function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = Input(\n",
    "    name=\"input_data\", \n",
    "    data_type=\"json\"\n",
    ")\n",
    "\n",
    "model_input = Input(\n",
    "    name=\"input_model_path\", \n",
    "    data_type=\"string\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output the name must be a key in the dictionary returned by your pipeline's function. In our case, name=\"predictions\" as in the last line of function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = Output(\n",
    "    name=\"predictions\",\n",
    "    data_type=\"json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything we need to create, as before, the pipeline corresponding to our new ``predictIris()`` function.\n",
    "This is exclatly like in part 2 except for two parameters :\n",
    "\n",
    "- inputs containing the list of Input objects we declared above (here, prediction_input and model_input).\n",
    "- outputs containing the list of Output objects we declared above (here, prediction_output).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk.create_pipeline(\n",
    "    pipeline_name=\"part-3-irisio\",\n",
    "    function_path=\"src/part-3-iris-predict.py\",\n",
    "    function_name=\"predictIris\", \n",
    "\tdescription=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\n",
    "\tcontainer_config={\n",
    "        \"local_folder\": \"../../get_started\",\n",
    "        \"requirements_path\": \"requirements.txt\",\n",
    "        },\n",
    "    inputs=[prediction_input, model_input],\n",
    "    outputs=[prediction_output],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_list = sdk.list_pipelines()\n",
    "pipeline_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pipeline information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_info = sdk.get_pipeline(\"part-3-irisio\")\n",
    "pipeline_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline (RUN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to execute the pipeline we need data as input (formatted as we said above). \n",
    "\n",
    "Let’s prepare it, simply by choosing some of the rows of iris dataset we did not use when training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(150)\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "iris_X_test = iris_X.loc[indices[90:120],:]\n",
    "\n",
    "new_data = iris_X_test.to_dict(orient=\"index\")\n",
    "new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to encapsulate this dictionary in another dictionary whose keys are \"input_data\" and \"input_model_path\" (the names of the inputs of our pipeline, i.e. the names of the arguments of our pipeline's function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "\t\"input_data\": new_data,\n",
    "    \"input_model_path\": \"get_started/models/iris_knn_model.joblib\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline with inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our pipeline execution with the data we’ve just prepared by calling the sdk.run_pipeline() function almost as in Part 2 except this time we will pass our inputs dictionary in the inputs arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions = sdk.run_pipeline(pipeline_name=\"part-3-irisio\", inputs=inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the return of our function but getting the item corresponding to the 'predictions' key in the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions['outputs']['predictions']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can check the logs of this execution directly on the platform interface or as follows, as in the previous parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_executions = sdk.list_pipeline_executions(pipeline_name=\"part-3-irisio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = sdk.get_pipeline_execution_logs(execution_id=pipeline_executions[-1]['execution_id'])\n",
    "\n",
    "print('\\n'.join(log[\"message\"] for log in logs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jul 29 2024, 17:02:10) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
