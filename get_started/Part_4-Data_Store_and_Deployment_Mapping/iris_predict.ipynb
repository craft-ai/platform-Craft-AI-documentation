{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Deploy a ML use case with inputs and outputs "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 3, we have built and run our second ML pipeline to retrieve our trained model from the data store, give some new data to it as our pipeline input and retrieve the result as an ouput of our pipeline execution.\n",
    "\n",
    "What if we want to let an external user execute our pipeline ? Or if we want to execute our pipeline on a certain schedule ?\n",
    "\n",
    "⇒ We need to deploy our pipeline via an **endpoint** and a **periodic deployment**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from craft_ai_sdk import CraftAiSdk, Input, Output, InputSource, OutputDestination\n",
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAFT_AI_SDK_TOKEN = os.environ.get(\"CRAFT_AI_SDK_TOKEN\")\n",
    "CRAFT_AI_ENVIRONMENT_URL = os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk = CraftAiSdk(sdk_token=CRAFT_AI_SDK_TOKEN, environment_url=CRAFT_AI_ENVIRONMENT_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare inputs and outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input the name corresponds to the name of an argument of your pipeline's function. In our case name=\"input_data\" and \"input_model\" (as in the first line of function).\n",
    "\n",
    "The input about the model is not a string anymore, it's now defined as a file (that we can to retrieve from the data store). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = Input(\n",
    "    name=\"input_data\", \n",
    "    data_type=\"json\"\n",
    ")\n",
    "\n",
    "model_input = Input(\n",
    "    name=\"input_model\", \n",
    "    data_type=\"file\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the same thing for the output, it is now a file (that we want to store on the data store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_output = Output(\n",
    "    name=\"predictions\",\n",
    "    data_type=\"json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the pipeline as in the previous parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk.create_pipeline(\n",
    "    pipeline_name=\"part-4-iris-deployment\",\n",
    "    function_path=\"src/part-4-iris-predict.py\",\n",
    "    function_name=\"predictIris\", \n",
    "\tdescription=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\n",
    "\tcontainer_config={\n",
    "        \"local_folder\": \"../../get_started\",\n",
    "        \"requirements_path\": \"requirements.txt\",\n",
    "        },\n",
    "    inputs=[prediction_input, model_input],\n",
    "    outputs=[prediction_output],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk.get_pipeline(pipeline_name=\"part-4-iris-deployment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the endpoint with input and output mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **Endpoint** is a publicly accessible URL that launches the execution of the Pipeline.\n",
    "\n",
    "Without the platform, you would need to write an api with a library like Flask, Fast API or Django and deploy it on a server that you would have to maintain.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IO Mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to be able to trigger the execution by the endpoint and not only by a run anymore. \n",
    "\n",
    "Now, we want the user to be able to use it:\n",
    "- to send the input data directly to the pipeline via the endpoint\n",
    "- to retrieve the results on the data store\n",
    " \n",
    "We want also to specify the path to the stored model on the data store, so that the endpoint will take this model directly in the data store. The user won't be the one selecting the model used, it's only on the technical side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mapping_endpoint = [\n",
    "    InputSource(\n",
    "        pipeline_input_name=\"input_model\",\n",
    "        datastore_path=\"get_started/models/iris_knn_model.joblib\"\n",
    "        ),\n",
    "    InputSource(\n",
    "        pipeline_input_name=\"input_data\", \n",
    "        endpoint_input_name=\"input_data\"\n",
    "        )\n",
    "]\n",
    "\n",
    "output_mapping_endpoint = [\n",
    "    OutputDestination(\n",
    "        pipeline_output_name=\"predictions\",\n",
    "        endpoint_output_name=\"iris_type\")\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the platform you can create an Endpoint a simple call to the sdk.create_deployment() function of the SDK, by choosing an endpoint_name. This name is used to reference the created endpoint and is further used in its URL. \n",
    "\n",
    "For each deployment you can chose either to use the `elastic` mode or the `low-latency` mode. \n",
    "\n",
    "- The `low-latency` mode is used for real-time executions, a part of the environnement is already reserved for this deployement's executions. \n",
    "- The `elastic` mode is used when there is no need for real-time executions, the next executions will be executed on any avaialble part on the environnement.\n",
    "\n",
    "By default, the `elastic` mode is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = sdk.create_deployment(\n",
    "    execution_rule=\"endpoint\",\n",
    "    pipeline_name=\"part-4-iris-deployment\",\n",
    "    deployment_name=\"part-4-iris-endpoint\",\n",
    "    mode=\"low_latency\",\n",
    "    inputs_mapping=inputs_mapping_endpoint,\n",
    "    outputs_mapping=output_mapping_endpoint\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target the endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare some input data as in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(150)\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "iris_X_test = iris_X.loc[indices[90:120],:]\n",
    "\n",
    "new_data = iris_X_test.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your endpoint is created, you can execute it with a direct HTTP call with the endpoint token. Calling/triggering an endpoint allows us to execute the associated pipeline. Using Python, it can be executed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = sdk.base_environment_url  + \"/endpoints/\" + endpoint[\"name\"]\n",
    "\n",
    "inputs = {\"input_data\": new_data}\n",
    "endpoint_token = endpoint[\"endpoint_token\"]\n",
    "\n",
    "request = requests.post(endpoint_url, headers={\"Authorization\": f\"EndpointToken {endpoint_token}\"}, json=inputs)\n",
    "request.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP code 200 indicates that the request has been taken into account. In case of an error, we can expect an error code starting with 4XX or 5XX.\n",
    "\n",
    "But, obviously, you can execute it in any other way (curl command in bash, Postman…)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re train your model periodically"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine that our dataset is frequently updated, for instance we get new labeled iris data every day. In this case we might want to retrain our model by triggering our training pipeline `part4-iris-train` every day. The platform can do this automatically using the `periodic` execution rule in our deployment.\n",
    "\n",
    "A **periodic** deployment allows a pipeline to be executed at a certain schedule. For example, every monday at a certain time, every month, every 5 minutes etc.\n",
    "\n",
    "The inputs and outputs have to be defined, with a constant value or a mapping to the data store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will update our trainIris function so that it produces a file output containing our model, that we will then map to the datastore. Check the updated version of this function in `src/part-4-iris-predict.py`\n",
    "\n",
    "We can thet create the pipeline as we are used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = Output(\n",
    "    name=\"model\",\n",
    "    data_type=\"file\"\n",
    ")\n",
    "\n",
    "sdk.create_pipeline(\n",
    "    pipeline_name=\"part-4-iristrain\",\n",
    "    function_path=\"src/part-4-iris-predict.py\",\n",
    "    function_name=\"trainIris\",\n",
    "    container_config={\n",
    "        \"local_folder\": \"../../get_started\",\n",
    "        \"requirements_path\": \"requirements.txt\",\n",
    "        },\n",
    "    outputs=[train_output],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a deployment that executes our pipeline every 5 minutes. In our case, we will map the prediction output (which is our only I/O) to the datastore on the same path that is used in the pediction endpoint deployment. This way our prediction pipeline will automatically use the last version of our model for predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the schedule argument takes the CRON syntax (examples here: https://crontab.guru/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mapping_periodic = OutputDestination(\n",
    "    pipeline_output_name=\"model\",\n",
    "    datastore_path=\"get_started/models/iris_knn_model.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic = sdk.create_deployment(\n",
    "    execution_rule=\"periodic\",\n",
    "\tpipeline_name=\"part-4-iristrain\",\n",
    "\tdeployment_name=\"part-4-iristrain\",\n",
    "    schedule=\"*/5 * * * *\",\n",
    "    outputs_mapping=[output_mapping_periodic],\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training pipeline will now be executed every 5 minutes, updating our model with the potential new data. The predict pipeline will then use this updated model automatically.\n",
    "\n",
    "You can check that you actually have a new execution every 5 minutes using the sdk or via the web interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jul 29 2024, 17:02:10) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
